{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Learn Sampling pattern with multi-resolution\n\nA small pytorch example to showcase learning k-space sampling patterns.\nThis example showcases the auto-diff capabilities of the NUFFT operator \nwrt to k-space trajectory in mri-nufft.\n\nIn this example we learn the k-space samples $\\mathbf{K}$ for the following cost function:\n\n\\begin{align}\\mathbf{\\hat{K}} =  arg \\min_{\\mathbf{K}} ||  \\mathcal{F}_\\mathbf{K}^* D_\\mathbf{K} \\mathcal{F}_\\mathbf{K} \\mathbf{x} - \\mathbf{x} ||_2^2\\end{align}\nwhere $\\mathcal{F}_\\mathbf{K}$ is the forward NUFFT operator and $D_\\mathbf{K}$ is the density compensators for trajectory $\\mathbf{K}$,  $\\mathbf{x}$ is the MR image which is also the target image to be reconstructed.\n\nAdditionally, in-order to converge faster, we also learn the trajectory in a multi-resolution fashion. This is done by first optimizing a 8 times decimated trajectory locations, called control points. After a fixed number of iterations (5 in this example), these control points are upscaled by a factor of 2. However, note that the NUFFT operator always holds linearly interpolated version of the control points as k-space sampling trajectory.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This example can run on a binder instance as it is purely CPU based backend (finufft), and is restricted to a 2D single coil toy case.</p></div>\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>This example only showcases the autodiff capabilities, the learned sampling pattern is not scanner compliant as the scanner gradients required to implement it violate the hardware constraints. In practice, a projection $\\Pi_\\mathcal{Q}(\\mathbf{K})$ into the scanner constraints set $\\mathcal{Q}$ is recommended (see [Proj]_). This is implemented in the proprietary SPARKLING package [Sparks]_. Users are encouraged to contact the authors if they want to use it.</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-info'>\n\n# Install libraries needed for Colab\n\nThe below installation commands are needed to be run only on Google Colab.\n</div>\n<div class=\"colab-button\">\n            <a href=\"https://colab.research.google.com/github/mind-inria/mri-nufft/blob/gh-pages/examples/generated/autoexamples/example_learn_samples_multires.ipynb\" target=\"_blank\">\n                <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" \n                alt=\"Open In Colab\"/>\n            </a>\n        </div>\n        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Install libraries\n!pip install mri-nufft[finufft]\npip install brainweb-dl  # Required for data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nimport joblib\n\nimport brainweb_dl as bwdl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nfrom PIL import Image, ImageSequence\n\nfrom mrinufft import get_operator\nfrom mrinufft.trajectories import initialize_2D_radial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup a simple class to learn trajectory\n<div class=\"alert alert-info\"><h4>Note</h4><p>While we are only learning the NUFFT operator, we still need the gradient `wrt_data=True` to have all the gradients computed correctly.\n    See [Projector]_ for more details.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n    def __init__(\n        self,\n        inital_trajectory,\n        img_size=(256, 256),\n        start_decim=8,\n        interpolation_mode=\"linear\",\n    ):\n        super(Model, self).__init__()\n        self.control = torch.nn.Parameter(\n            data=torch.Tensor(inital_trajectory[:, ::start_decim]),\n            requires_grad=True,\n        )\n        self.current_decim = start_decim\n        self.interpolation_mode = interpolation_mode\n        sample_points = inital_trajectory.reshape(-1, inital_trajectory.shape[-1])\n        self.operator = get_operator(\"finufft\", wrt_data=True, wrt_traj=True)(\n            sample_points,\n            shape=img_size,\n            squeeze_dims=False,\n        )\n        self.img_size = img_size\n\n    def _interpolate(self, traj, factor=2):\n        \"\"\"Torch interpolate function to upsample the trajectory\"\"\"\n        return torch.nn.functional.interpolate(\n            traj.moveaxis(1, -1),\n            scale_factor=factor,\n            mode=self.interpolation_mode,\n            align_corners=True,\n        ).moveaxis(-1, 1)\n\n    def get_trajectory(self):\n        \"\"\"Function to get trajectory, which is interpolated version of control points.\"\"\"\n        traj = self.control.clone()\n        for i in range(np.log2(self.current_decim).astype(int)):\n            traj = self._interpolate(traj)\n\n        return traj.reshape(-1, traj.shape[-1])\n\n    def upscale(self, factor=2):\n        \"\"\"Upscaling the model.\n        In this step, the number of control points are doubled and interpolated.\n        \"\"\"\n        self.control = torch.nn.Parameter(\n            data=self._interpolate(self.control),\n            requires_grad=True,\n        )\n        self.current_decim /= factor\n\n    def forward(self, x):\n        traj = self.get_trajectory()\n        self.operator.samples = traj\n\n        # Simulate the acquisition process\n        kspace = self.operator.op(x)\n\n        adjoint = self.operator.adj_op(kspace).abs()\n        return adjoint / torch.mean(adjoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Util function to plot the state of the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_state(\n    axs, mri_2D, traj, recon, control_points=None, loss=None, save_name=None\n):\n    axs = axs.flatten()\n    axs[0].imshow(np.abs(mri_2D[0]), cmap=\"gray\")\n    axs[0].axis(\"off\")\n    axs[0].set_title(\"MR Image\")\n    axs[1].scatter(*traj.T, s=0.5)\n    if control_points is not None:\n        axs[1].scatter(*control_points.T, s=1, color=\"r\")\n        axs[1].legend([\"Trajectory\", \"Control Points\"])\n    axs[1].set_title(\"Trajectory\")\n    axs[2].imshow(np.abs(recon[0][0].detach().cpu().numpy()), cmap=\"gray\")\n    axs[2].axis(\"off\")\n    axs[2].set_title(\"Reconstruction\")\n    if loss is not None:\n        axs[3].plot(loss)\n        axs[3].grid(\"on\")\n        axs[3].set_title(\"Loss\")\n    if save_name is not None:\n        plt.savefig(save_name, bbox_inches=\"tight\")\n        plt.close()\n    else:\n        plt.show()\n\n\ndef upsample_optimizer(optimizer, new_optimizer, factor=2):\n    \"\"\"Upsample the optimizer.\"\"\"\n    for old_group, new_group in zip(optimizer.param_groups, new_optimizer.param_groups):\n        for old_param, new_param in zip(old_group[\"params\"], new_group[\"params\"]):\n            # Interpolate optimizer states\n            if old_param in optimizer.state:\n                for key in optimizer.state[old_param].keys():\n                    if isinstance(optimizer.state[old_param][key], torch.Tensor):\n                        old_state = optimizer.state[old_param][key]\n                        if old_state.ndim == 0:\n                            new_state = old_state\n                        else:\n                            new_state = torch.nn.functional.interpolate(\n                                old_state.moveaxis(1, -1),\n                                scale_factor=factor,\n                                mode=\"linear\",\n                            ).moveaxis(-1, 1)\n                        new_optimizer.state[new_param][key] = new_state\n                    else:\n                        new_optimizer.state[new_param][key] = optimizer.state[\n                            old_param\n                        ][key]\n    return new_optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Inputs (models, trajectory and image)\nFirst we create the model with a simple radial trajectory (32 shots of 256 points)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "init_traj = initialize_2D_radial(32, 256).astype(np.float32)\nmodel = Model(init_traj, img_size=(256, 256))\nmodel.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The image on which we are going to train.\n.. note ::\n   In practice we would use instead a dataset (e.g. fastMRI)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mri_2D = torch.from_numpy(np.flipud(bwdl.get_mri(4, \"T1\")[80, ...]).astype(np.float32))[\n    None\n]\nmri_2D = mri_2D / torch.mean(mri_2D)\n\n\n# Initialisation\n# --------------\n# Before training, here is the simple reconstruction we have using a\n# density compensated adjoint.\n\nrecon = model(mri_2D)\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\nplot_state(axs, mri_2D, init_traj, recon, model.control.detach().cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start training loop\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nlosses = []\nimage_files = []\nmodel.train()\nwhile model.current_decim >= 1:\n    with tqdm(range(30), unit=\"steps\") as tqdms:\n        for i in tqdms:\n            out = model(mri_2D)\n            loss = torch.nn.functional.mse_loss(out, mri_2D[None, None])\n            numpy_loss = (loss.detach().cpu().numpy(),)\n\n            tqdms.set_postfix({\"loss\": numpy_loss})\n            losses.append(numpy_loss)\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n            with torch.no_grad():\n                # Clamp the value of trajectory between [-0.5, 0.5]\n                for param in model.parameters():\n                    param.clamp_(-0.5, 0.5)\n            # Generate images for gif\n            hashed = joblib.hash((i, \"learn_traj\", time.time()))\n            filename = \"/tmp/\" + f\"{hashed}.png\"\n            plt.clf()\n            fig, axs = plt.subplots(2, 2, figsize=(10, 10), num=1)\n            plot_state(\n                axs,\n                mri_2D,\n                model.get_trajectory().detach().cpu().numpy(),\n                out,\n                model.control.detach().cpu().numpy(),\n                losses,\n                save_name=filename,\n            )\n            image_files.append(filename)\n        if model.current_decim == 1:\n            break\n        else:\n            model.upscale()\n            optimizer = upsample_optimizer(\n                optimizer, torch.optim.Adam(model.parameters(), lr=1e-3)\n            )\n\n\n# Make a GIF of all images.\nimgs = [Image.open(img) for img in image_files]\nimgs[0].save(\n    \"mrinufft_learn_traj_multires.gif\",\n    save_all=True,\n    append_images=imgs[1:],\n    optimize=False,\n    duration=2,\n    loop=0,\n)\n\n# sphinx_gallery_thumbnail_path = 'generated/autoexamples/images/mrinufft_learn_traj_multires.gif'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. image-sg:: /generated/autoexamples/images/mrinufft_learn_traj_multires.gif\n   :alt: example learn_samples\n   :srcset: /generated/autoexamples/images/mrinufft_learn_traj_multires.gif\n   :class: sphx-glr-single-img\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trained trajectory\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.eval()\nrecon = model(mri_2D)\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\nplot_state(\n    axs,\n    mri_2D,\n    model.get_trajectory().detach().cpu().numpy(),\n    recon=recon,\n    control_points=None,\n    loss=losses,\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The above learned trajectory is not that good because:\n   - The trajectory is trained only for 5 iterations per decimation level, resulting in a suboptimal trajectory.\n   - In order to make the example CPU compliant, we had to resort to preventing density compensation, hence the reconstructor is not good.</p></div>\n\nUsers are requested to checkout `sphx_glr_generated_autoexamples_GPU_example_learn_samples.py` for example with density compensation.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n\n.. [Proj] N. Chauffert, P. Weiss, J. Kahn and P. Ciuciu, \"A Projection Algorithm for\n          Gradient Waveforms Design in Magnetic Resonance Imaging,\" in\n          IEEE Transactions on Medical Imaging, vol. 35, no. 9, pp. 2026-2039, Sept. 2016,\n          doi: 10.1109/TMI.2016.2544251.\n.. [Sparks] G. R. Chaithya, P. Weiss, G. Daval-Frérot, A. Massire, A. Vignaud and P. Ciuciu,\n          \"Optimizing Full 3D SPARKLING Trajectories for High-Resolution Magnetic\n          Resonance Imaging,\" in IEEE Transactions on Medical Imaging, vol. 41, no. 8,\n          pp. 2105-2117, Aug. 2022, doi: 10.1109/TMI.2022.3157269.\n.. [Projector] Chaithya GR, and Philippe Ciuciu. 2023. \"Jointly Learning Non-Cartesian\n          k-Space Trajectories and Reconstruction Networks for 2D and 3D MR Imaging\n          through Projection\" Bioengineering 10, no. 2: 158.\n          https://doi.org/10.3390/bioengineering10020158\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}